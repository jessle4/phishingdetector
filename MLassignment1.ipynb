{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2025 Semester 1\n",
    "\n",
    "## Assignment 1: Scam detection with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Student ID(s): 1353984\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, GRAPHS, AND FIGURES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).** Results, figures, etc. which appear in this file but are NOT included in your report will not be marked.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load training data\n",
    "train_df = pd.read_csv('/Users/jessicale/Desktop/machinelearning/COMP30027_2025_asst1_data/sms_supervised_train.csv')\n",
    "\n",
    "# focus opn preprocessed text and class label\n",
    "train_texts = train_df['textPreprocessed']\n",
    "train_labels = train_df['class']\n",
    "\n",
    "# remove out the NaN and non string values and replace with an empty string\n",
    "train_texts = train_df['textPreprocessed'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2006\n"
     ]
    }
   ],
   "source": [
    "#building vocab and print out number of words\n",
    "vocab = set()\n",
    "for text in train_texts:\n",
    "    words = text.split() # splitting words by the white spaces\n",
    "    vocab.update(words)\n",
    "vocab = sorted(list(vocab))\n",
    "vocab_size = len(vocab)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count matrix shape: (2000, 2006)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#creating the count matrix\n",
    "import numpy as np\n",
    "\n",
    "#create map of word to index\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "#inir count matrix where rows represent messages and columns represents words\n",
    "num_messages = len(train_texts)\n",
    "count_matrix= np.zeros((num_messages, vocab_size), dtype = int)\n",
    "\n",
    "# fill in matrix\n",
    "for i, text in enumerate(train_texts):\n",
    "    for word in text.split():\n",
    "        if word in word_to_idx:\n",
    "            count_matrix[i, word_to_idx[word]] += 1\n",
    "\n",
    "print(\"Count matrix shape:\", count_matrix.shape)\n",
    "print(count_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.8\n",
      "1:0.2\n"
     ]
    }
   ],
   "source": [
    "# compute prior probabilties of each class\n",
    "\n",
    "# count the amount of instances belong to each class\n",
    "classes = train_labels.unique()\n",
    "\n",
    "# create dictionary to store prior prob of the classes\n",
    "priors = {}\n",
    "num_total_messages = len(train_texts)\n",
    "\n",
    "for c in classes:\n",
    "    # add the number of messages in that class\n",
    "    count = (train_labels == c).sum()\n",
    "    #calculates prior probabilties by dividng count by total messages\n",
    "    priors[c] = count / num_total_messages\n",
    "    print(f\"{c}:{priors[c]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.01718733 0.00559848 0.00537454 0.00044788 0.00016795 0.01310044\n",
      " 0.00391893 0.00061583 0.00011197 0.00033591]\n"
     ]
    }
   ],
   "source": [
    "#calculating likelihooods of each word appearing in a message of class c with laplace smoothing\n",
    "likelihoods = {}\n",
    "\n",
    "#laplace smoothing parameter\n",
    "alpha = 1\n",
    "\n",
    "for c in classes:\n",
    "    class_indices = (train_labels == c)\n",
    "    class_counts = count_matrix[class_indices]\n",
    "\n",
    "    #total count of all the words in class c (which is either scam or non-malicious\n",
    "    total_count = class_counts.sum()\n",
    "\n",
    "    #sum counts for each word in the vocab (columns)\n",
    "    word_counts = class_counts.sum(axis = 0)\n",
    "\n",
    "    #apply laplace smoothing to find likelihoods of each word\n",
    "    likelihood = (word_counts + alpha) / (total_count + vocab_size * alpha)\n",
    "    likelihoods[c] = likelihood\n",
    "\n",
    "# see fgirs ten likelikhoods for one class\n",
    "print(classes[0], likelihoods[classes[0]][:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words for class 0:\n",
      " .: 0.0793\n",
      " ,: 0.0260\n",
      " ?: 0.0256\n",
      " u: 0.0189\n",
      " ...: 0.0188\n",
      " !: 0.0172\n",
      " ..: 0.0149\n",
      " ;: 0.0132\n",
      " &: 0.0131\n",
      " go: 0.0111\n",
      "\n",
      "Top words for class 1:\n",
      " .: 0.0565\n",
      " !: 0.0244\n",
      " ,: 0.0235\n",
      " call: 0.0205\n",
      " £: 0.0139\n",
      " free: 0.0105\n",
      " /: 0.0091\n",
      " 2: 0.0088\n",
      " &: 0.0087\n",
      " ?: 0.0085\n"
     ]
    }
   ],
   "source": [
    "#finding the 10 most probable words per class and prob values of each \n",
    "\n",
    "def most_probable(likelihood, vocab):\n",
    "    #get indexes of the top 10 likelihoods\n",
    "    top10 = np.argsort(likelihood)[-10:][::-1]\n",
    "    return[(vocab[i], likelihood[i]) for i in top10]\n",
    "\n",
    "for c in classes:\n",
    "    top_words = most_probable(likelihoods[c], vocab)\n",
    "    print(f\"\\nTop words for class {c}:\")\n",
    "    for word, prob in top_words:\n",
    "        print(f\" {word}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top 10 words predictive of scam:\n",
      "prize: 99.0284\n",
      "tone: 64.0772\n",
      "£: 49.7084\n",
      "select: 46.6016\n",
      "claim: 45.9543\n",
      "paytm: 36.8929\n",
      "code: 34.9512\n",
      "award: 32.0386\n",
      "won: 31.0677\n",
      "18: 29.1260\n",
      "\n",
      "top 10 words predictive of non-malicious:\n",
      ";: 60.5130\n",
      "...: 57.5088\n",
      "gt: 54.0754\n",
      "lt: 53.5604\n",
      ":): 47.8954\n",
      "ü: 31.9302\n",
      "lor: 28.8402\n",
      "hope: 24.7202\n",
      "ok: 24.7202\n",
      "d: 21.1152\n"
     ]
    }
   ],
   "source": [
    "# most predictive wordsof each class \n",
    "scam_class = 1\n",
    "okay_class  = 0 # non malicious\n",
    "\n",
    "\n",
    "ratios = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    ratio = likelihoods[scam_class][i] / likelihoods[okay_class][i]\n",
    "    ratios[word] = ratio\n",
    "\n",
    "# sort words by highest ratio for scam predictive words\n",
    "top_scam_predictions = sorted(ratios.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\ntop 10 words predictive of scam:\")\n",
    "for word, ratio in top_scam_predictions:\n",
    "    print(f\"{word}: {ratio:.4f}\")\n",
    "\n",
    "#for non-malicious words, the inverse ratios are found \n",
    "ratios_okay = {word: 1/ratio for word, ratio in ratios.items()}\n",
    "top_okay_predictions = sorted(ratios_okay.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\ntop 10 words predictive of non-malicious:\")\n",
    "for word, ratio in top_okay_predictions:\n",
    "    print(f\"{word}: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def classify_message(message, priors, likelihoods, word_to_idx, vocab):\n",
    "    # build vector for the message being analysed\n",
    "    counts = np.zeros(len(vocab))\n",
    "    for word in message.split():\n",
    "        if word in word_to_idx:\n",
    "            counts[word_to_idx[word]] += 1\n",
    "\n",
    "    # if there's no known word, skip\n",
    "    if counts.sum() == 0:\n",
    "        return None, None\n",
    "\n",
    "    log_posteriors = {}\n",
    "\n",
    "    for c in priors:\n",
    "        # start with log prior\n",
    "        log_prob = math.log(priors[c])\n",
    "\n",
    "        # add log likelihoods for each word\n",
    "        for i, count in enumerate(counts):\n",
    "            if count > 0:\n",
    "                log_prob += count * math.log(likelihoods[c][i])\n",
    "\n",
    "        # ✅ This line must be inside the `for c in priors` loop!\n",
    "        log_posteriors[c] = log_prob\n",
    "\n",
    "    predicted_class = max(log_posteriors, key=log_posteriors.get)\n",
    "    return predicted_class, log_posteriors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of test messages skipped: 0\n",
      "accuracy on test set: 0.975\n",
      "confusion Matrix:\n",
      " [[785  15]\n",
      " [ 10 190]]\n",
      "classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       800\n",
      "           1       0.93      0.95      0.94       200\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.96      0.97      0.96      1000\n",
      "weighted avg       0.98      0.97      0.98      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "test_df = pd.read_csv('/Users/jessicale/Desktop/machinelearning/COMP30027_2025_asst1_data/sms_test.csv')\n",
    "test_texts = test_df['textPreprocessed']\n",
    "test_labels = test_df['class'] \n",
    "test_texts = test_df['textPreprocessed'].fillna('').astype(str)\n",
    "\n",
    "predictions = []\n",
    "skipped = 0\n",
    "for text in test_texts:\n",
    "    pred, _ = classify_message(text, priors, likelihoods, word_to_idx, vocab)\n",
    "    if pred is None:\n",
    "        skipped += 1\n",
    "        predictions.append(None)\n",
    "    else:\n",
    "        predictions.append(pred)\n",
    "\n",
    "print(\"number of test messages skipped:\", skipped)\n",
    "\n",
    "# compute accuracy and confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# filter out skipped messages if there are any skipped\n",
    "valid_idx = [i for i, p in enumerate(predictions) if p is not None]\n",
    "valid_preds = [predictions[i] for i in valid_idx]\n",
    "valid_true = test_labels.iloc[valid_idx]\n",
    "\n",
    "acc = accuracy_score(valid_true, valid_preds)\n",
    "cm = confusion_matrix(valid_true, valid_preds)\n",
    "print(\"accuracy on test set:\", acc)\n",
    "print(\"confusion Matrix:\\n\", cm)\n",
    "print(\"classification Report:\\n\", classification_report(valid_true, valid_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High confidence scam examples:\n",
      "R = 30464191.5694 : 1 . . reply free free , , , message 's yes text : hello orange month access game news sport plus 10 20 photo term apply\n",
      "R = 483730.3741 : . ! call customer important service announcement freephone 0800 542\n",
      "R = 2426496.5084 : . . 4 send : win easy just question hmv hmv hmv bonus special 500 pound vouchers answer 86688 info\n",
      "R = 14375010869.1781 : . . . . u u send know know year ! / / / / find msg 86688 150p 18 rcvd chat hg suite342/2lands row w1j6hl ldn\n",
      "R = 13.8230 : ? reply single single break plus answer fight\n",
      "R = 156993185172664064.0000 : ? . please call award end 350 todays voda number select receive match 08712300220 quoting claim code standard rate app\n",
      "R = 156891569358454.4688 : please call rs. guarantee customer prize service team fl1pkart w0n representative 6200992462 10am-9pm cash\n",
      "R = 8296831.6617 : ? . 4 4 u week £ txt new question name 100 draw thanks enter cash continue support an\n",
      "R = 90446758236257714176.0000 : . call £ £ guarantee won customer prize prize claim service 1000 yr 2000 representative cash 10am-7pm\n",
      "R = 184952.8766 : ? . reply send stop stop ( ) / invite friend : frnd 62468 m\n",
      "\n",
      "High confidence non-malicious examples:\n",
      "R = 0.0002 : ? ? up come online free talk sometime �\n",
      "R = 0.0000 : ? . . up know quite still get hold anyone cud pick bout they 're pub\n",
      "R = 0.0001 : .. , house-maid murderer coz man murder 26th january\n",
      "R = 0.0000 : ? . 2 come u get ok thk wan wat\n",
      "R = 0.0000 : lor wan ... ... ... e oh tt tt den take end cine dun yogasana oso\n",
      "R = 0.0063 : .. come free , , ur il download wen\n",
      "R = 0.0006 : ? ? u u good ! ! hey mate honey ave holiday de x\n",
      "R = 0.0000 : , back 's text time take just derek & amp ; leave ready\n",
      "R = 0.0000 : .. ok let knw\n",
      "R = 0.0082 : . . lady bad girl\n",
      "\n",
      "Boundary examples:\n",
      "R = 0.9825 : . call dear\n",
      "R = 0.9571 : . reply glad\n",
      "R = 1.0783 : ? ur * just alrite sam\n"
     ]
    }
   ],
   "source": [
    "def compute_confidence_ratio(log_posteriors, scam_class=1, okay_class=0):\n",
    "    # compute ratio in the original probability space\n",
    "    # R = exp(log(P(scam|x)) - log(P(non-malicious|x)))\n",
    "    ratio = math.exp(log_posteriors[scam_class] - log_posteriors[okay_class])\n",
    "    return ratio\n",
    "\n",
    "# gather examples for different confidence levels\n",
    "high_conf_scam = []\n",
    "high_conf_okay = []\n",
    "boundary = []\n",
    "\n",
    "for text in test_texts:\n",
    "    pred, log_post = classify_message(text, priors, likelihoods, word_to_idx, vocab)\n",
    "    # skip messages that dont have any words in the vocab\n",
    "    if pred is None:\n",
    "        continue\n",
    "    ratio = compute_confidence_ratio(log_post, scam_class=1, okay_class=0)\n",
    "    \n",
    "    # defining what high confidence scam and nonmalicious texts are \n",
    "    if pred == 1 and ratio > 10:\n",
    "        high_conf_scam.append((text, ratio))\n",
    "    elif pred == 0 and ratio < 0.1:\n",
    "        high_conf_okay.append((text, ratio))\n",
    "    elif 0.9 < ratio < 1.1:\n",
    "        boundary.append((text, ratio))\n",
    "    \n",
    "    # stop after three examples of high confidence \n",
    "    if len(high_conf_scam) >= 3 and len(high_conf_okay) >= 3 and len(boundary) >= 3:\n",
    "        break\n",
    "\n",
    "print(\"High confidence scam examples:\")\n",
    "for text, r in high_conf_scam[:10]:\n",
    "    print(f\"R = {r:.4f} : {text}\")\n",
    "\n",
    "print(\"\\nHigh confidence non-malicious examples:\")\n",
    "for text, r in high_conf_okay[:10]:\n",
    "    print(f\"R = {r:.4f} : {text}\")\n",
    "\n",
    "print(\"\\nBoundary examples:\")\n",
    "for text, r in boundary[:10]:\n",
    "    print(f\"R = {r:.4f} : {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending the model with semi-supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTEMPTING LABEL PROPAGRATION OPTION\n",
    "\n",
    "#load the unlabelled dataset\n",
    "unlabelled_df = pd.read_csv('/Users/jessicale/Desktop/machinelearning/COMP30027_2025_asst1_data/sms_unlabelled.csv')\n",
    "unlabelled_texts = unlabelled_df['textPreprocessed']\n",
    "unlabelled_texts = unlabelled_texts.dropna() #ignore null or nonstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1858 high-confidence pseudo-labelled instances.\n"
     ]
    }
   ],
   "source": [
    "# classifying unsupervised dataset using classify_message from Q1\n",
    "\n",
    "pseudo_labels = []\n",
    "pseudo_texts = []\n",
    "confidence_threshold = 10\n",
    "\n",
    "for text in unlabelled_texts:\n",
    "    pred, log_post = classify_message(text, priors, likelihoods, word_to_idx, vocab)\n",
    "    if pred is None:\n",
    "        continue  \n",
    "    ratio = compute_confidence_ratio(log_post)\n",
    "    \n",
    "    #keep high-confidence predictions\n",
    "    if ratio > confidence_threshold or ratio < 1 / confidence_threshold:\n",
    "        pseudo_labels.append(pred)\n",
    "        pseudo_texts.append(text)\n",
    "\n",
    "print(f\"Added {len(pseudo_labels)} high-confidence pseudo-labelled instances.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine supervised and unsupervised data sets\n",
    "\n",
    "# convert to series to align shapes\n",
    "pseudo_texts_series = pd.Series(pseudo_texts)\n",
    "pseudo_labels_series = pd.Series(pseudo_labels)\n",
    "\n",
    "# combine with original training data\n",
    "combined_texts = pd.concat([train_texts, pseudo_texts_series], ignore_index=True)\n",
    "combined_labels = pd.concat([train_labels, pseudo_labels_series], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuilding vocav and the count matrix\n",
    "\n",
    "# cuild new vocab\n",
    "combined_vocab = sorted(set(word for text in combined_texts for word in text.split()))\n",
    "word_to_idx_combined = {word: i for i, word in enumerate(combined_vocab)}\n",
    "V_combined = len(combined_vocab)\n",
    "\n",
    "# cuild new count matrix\n",
    "N_combined = len(combined_texts)\n",
    "count_matrix_combined = np.zeros((N_combined, V_combined), dtype=int)\n",
    "\n",
    "for i, text in enumerate(combined_texts):\n",
    "    for word in text.split():\n",
    "        if word in word_to_idx_combined:\n",
    "            count_matrix_combined[i, word_to_idx_combined[word]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recompute priors and likelihoods\n",
    "\n",
    "# recalculate priors\n",
    "combined_classes = combined_labels.unique()\n",
    "combined_priors = {}\n",
    "for c in combined_classes:\n",
    "    combined_priors[c] = (combined_labels == c).sum() / N_combined\n",
    "\n",
    "# recalculate likelihoods\n",
    "alpha = 1\n",
    "combined_likelihoods = {}\n",
    "\n",
    "for c in combined_classes:\n",
    "    class_indices = (combined_labels == c)\n",
    "    class_counts = count_matrix_combined[class_indices]\n",
    "    total_count = class_counts.sum()\n",
    "    word_counts = class_counts.sum(axis=0)\n",
    "    \n",
    "    likelihood = (word_counts + alpha) / (total_count + V_combined * alpha)\n",
    "    combined_likelihoods[c] = likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 predictive words for scam (Q3):\n",
      "prize: R = 234.3411\n",
      "tone: R = 133.3320\n",
      "claim: R = 96.2953\n",
      "award: R = 77.7770\n",
      "code: R = 76.7669\n",
      "guaranteed: R = 74.7467\n",
      "£: R = 72.1494\n",
      "18: R = 70.7064\n",
      "paytm: R = 66.6660\n",
      "winner: R = 60.6055\n",
      "\n",
      "Top 10 predictive words for non-malicious (Q3):\n",
      ";: R = 0.0094\n",
      "gt: R = 0.0109\n",
      "lt: R = 0.0109\n",
      ":): R = 0.0122\n",
      "ü: R = 0.0168\n",
      "...: R = 0.0198\n",
      "lor: R = 0.0206\n",
      "da: R = 0.0217\n",
      "come: R = 0.0297\n",
      "wat: R = 0.0311\n"
     ]
    }
   ],
   "source": [
    "# grab likelihood vectors\n",
    "likelihood_scam = combined_likelihoods[1]\n",
    "likelihood_okay = combined_likelihoods[0]\n",
    "\n",
    "# compute ratios\n",
    "ratios = {}\n",
    "for i, word in enumerate(combined_vocab):\n",
    "    # avoid division by zero\n",
    "    ratio = likelihood_scam[i] / likelihood_okay[i]\n",
    "    ratios[word] = ratio\n",
    "\n",
    "# top 10 predictive words for scam (highest ratios)\n",
    "top_scam_words = sorted(ratios.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop 10 predictive words for scam (Q3):\")\n",
    "for word, r in top_scam_words:\n",
    "    print(f\"{word}: R = {r:.4f}\")\n",
    "\n",
    "# top 10 predictive words for non-malicious (lowest ratios)\n",
    "top_okay_words = sorted(ratios.items(), key=lambda x: x[1])[:10]\n",
    "print(\"\\nTop 10 predictive words for non-malicious (Q3):\")\n",
    "for word, r in top_okay_words:\n",
    "    print(f\"{word}: R = {r:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
